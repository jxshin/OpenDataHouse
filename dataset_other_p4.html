<!--
Copyright © 2019, empirical software engineering team from Peking Uninversity and ISCAS, All rights reserved.

Written by:
  Jiaxin Zhu
-->

<script src='js/header.js'></script>
   
<main role="main">
  <section class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-md-3">
          <div class="card my-4">
            <div class="card-header">
                Data Types
            </div>
            <ul class="list-group list-group-flush">
                <li class="list-group-item"><input type="checkbox"> VCS(0) </li> 
                <li class="list-group-item"><input type="checkbox"> ITS(0) </li> 
                <li class="list-group-item"><input type="checkbox"> Mails(0) </li> 
            </ul>   
          </div>
          <div class="card my-4">
            <div class="card-header">
                Data Sizes
            </div>
            <ul class="list-group list-group-flush">
                <li class="list-group-item"><input type="checkbox"> &lt;=500M(8) </li> 
                <li class="list-group-item"><input type="checkbox"> &gt;500M &lt;=1G(2) </li> 
                <li class="list-group-item"><input type="checkbox"> &gt;1G(30) </li> 
            </ul>
          </div>
          <div class="card my-4">
            <div class="card-header">
                Popularity
            </div>
            <ul class="list-group list-group-flush">
                <li class="list-group-item"><input type="checkbox"> &lt;=100(40) </li> 
                <li class="list-group-item"><input type="checkbox"> &gt;100 &lt;=500(0) </li> 
                <li class="list-group-item"><input type="checkbox"> &gt;500(0) </li> 
            </ul>
          </div>
        </div>
        <div class="col-md-9">
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/757717">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>Empirical validation of CodeCity - A controlled experiment</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Richard Wettel</a>;
                </span>
                
              </p>
              <p>
                ReferenceStudies who have been using the data /(in any 
form/) are required to include the following 
reference:@inproceedings{Wettel:2011:SSC:1985793.1985868, author = 
{Wettel, Richard and Lanza, Michele and Robbes, Romain}, title = 
{Software Systems As Cities: A Controlled Experiment}, booktitle = 
{Proceedings of the 33rd International Conference on Software 
Engineering}, series = {ICSE '11}, year = {2011}, isbn = 
{978-1-4503-0445-0}, location = {Waikiki, Honolulu, HI, USA}, pages = 
{551--560}, numpages = {10}, url = 
{http:////doi.acm.org//10.1145//1985793.1985868}, doi = 
{10.1145//1985793.1985868}, acmid = {1985868}, publisher = {ACM}, 
address = {New York, NY, USA}, keywords = {empirical validation, 
software visualization}, }About the DataOverview of DataThis paper 
provided the complete raw and processed data for the controlled 
experiment for the empirical evaluation of a 3D software visualization 
approach based on a city metaphor and implemented in a tool called 
CodeCity. This includes and is not limited to the pre-experiment, 
in-experiment and debriefing questionnaires, solution oracles and 
grading systems, correction scores and measured completion 
time.Attribute InformationDebriefing questionnaires, solution oracles 
and grading systems, correction scores and measured completion timePaper
 AbstractSoftware visualization is a popular program comprehension 
technique used in the context of software maintenance, reverse 
engineering, and software evolution analysis. While there is a broad 
range of software visualization approaches, only few have been 
empirically evaluated. This is detrimental to the acceptance of software
 visualization in both the academic and the industrial world. We present
 a controlled experiment for the empirical evaluation of a 3D software 
visualization approach based on a city metaphor and implemented in a 
tool called CodeCity. The goal is to provide experimental evidence of 
the viability of our approach in the context of program comprehension by
 having subjects perform tasks related to program comprehension. We 
designed our experiment based on lessons extracted from the current body
 of research. We conducted the experiment in four locations across three
 countries, involving 41 participants from both academia and industry. 
The experiment shows that CodeCity leads to a statistically significant 
increase in terms of task correctness and decrease in task completion 
time. We detail the experiment we performed, discuss its results and 
reflect on the many lessons learned.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/758349">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>cocommit</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Qi Xuan</a>;
                </span>
                
              </p>
              <p>
                ReferenceStudies who have been using the data /(in any 
form/) are required to add the following reference to their 
report//paper:@inproceedings{Xuan:2014:BTS:2568225.2568238, author = 
{Xuan, Qi and Filkov, Vladimir}, title = {Building It Together: 
Synchronous Development in OSS}, booktitle = {Proceedings of the 36th 
International Conference on Software Engineering}, series = {ICSE 2014},
 year = {2014}, isbn = {978-1-4503-2756-5}, location = {Hyderabad, 
India}, pages = {222--233}, numpages = {12}, url = 
{http:////doi.acm.org//10.1145//2568225.2568238}, doi = 
{10.1145//2568225.2568238}, acmid = {2568238}, publisher = {ACM}, 
address = {New York, NY, USA}, keywords = {OSS, collaboration, 
communication, synchronization}, }About the DataThe authors obtained 
data for 31 OSS projects from the Apache Software Foundation on March 
24th, 2012. For each project, the commit activities of developers on 
dierent files are gathered from the corresponding Git repository while 
the email communication activities are gathered from the online 
developer mailing lists. For each commit activity, The authors recorded 
the developer ID, file ID, file type, the exact submitting time in 
seconds, and the numbers of added and deleted LOC in each file. For each
 communication activity, The authors recorded the sender ID, receiver 
ID, and the sending time in seconds. Note that, developers may have 
multiple aliases, which were resolved by using a semi-automatic 
approach.AbstractIn distributed software development synchronized 
actions are important for completion of complex, interleaved tasks that 
require the abilities of multiple people. Synchronous development is 
manifested when le commits by two developers are close together in time 
and modify the same files. Here we propose quantitative methods for 
identifying synchronized activities in OSS projects, and use them to 
relate developer synchronization with effective productivity and 
communication. In particular, we dene co-commit bursts and communication
 bursts, as intervals of time rich in co-commit and correspondence 
activities, respectively, and construct from them smoothed time series 
which can be, subsequently, correlated to discover synchrony. We found 
that synchronized co-commits between developers are associated with 
their effective productivity and coordination: during co-commit bursts, 
vs. at other times, the project size grows faster even though the 
overall coding effort slows down. We also found strong correlation 
between synchronized co-commits and communication, that is, for pairs of
 developers,more co-commit bursts are accompanied with more 
communication bursts, and their relationship follows closely a linear 
model. In addition, synchronized co-commits and communication activities
 occur very close together in time, thus, they can also be thought of as
 synchronizing each other. This study can help with better understanding
 collaborative mechanisms in OSS and the role communication plays in 
distributed software engineering.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/2583977">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>The Software Heritage Graph Dataset</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Antoine Pietri</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Diomidis Spinellis</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Stefano Zacchiroli</a>;
                </span>
                
              </p>
              <p>
                Software Heritage is the largest existing public archive
 of software sourcecode and accompanying development history: it 
currently spans more than fivebillion unique source code files and one 
billion unique commits, coming frommore than 80 million software 
projects.This is the Software Heritage graph dataset: a 
fully-deduplicatedMerkle DAG representation of the Software Heritage 
archive. The dataset linkstogether file content identifiers, source code
 directories, Version ControlSystem /(VCS/) commits tracking evolution 
over time, up to the full states of VCSrepositories as observed by 
Software Heritage during periodic crawls. Thedataset/’s contents come 
from major development forges /(including GitHub andGitLab/), FOSS 
distributions /(e.g., Debian/), and language-specific packagemanagers 
/(e.g., PyPI/). /&nbsp;Crawling information is also included, 
providingtimestamps about when and where all archived source code 
artifacts have beenobserved in the wild.The Software Heritage graph 
dataset is available in multiple formats, includingdownloadable CSV 
dumps and Apache Parquet files for local use, as well as apublic 
instance on Amazon Athena interactive query service for 
ready-to-usepowerful analytical processing.By accessing the dataset, you
 agree with the Software Heritage/&nbsp;Ethical Charterfor using the 
archive data, and the/&nbsp;terms of use for bulk access.If you use this
 dataset for research purposes, please cite the following paper: Antoine
 Pietri, Diomidis Spinellis, Stefano Zacchiroli./&nbsp; The Software 
Heritage Graph Dataset: Public software development under one 
roof./&nbsp; In proceedings of/&nbsp;MSR 2019: The 16th International 
Conference on Mining Software Repositories, May 2019, Montreal, Canada. 
Co-located with/&nbsp;ICSE 2019./&nbsp; preprint,/&nbsp;bibtexYou can 
also refer to the above paper for more information the dataset and 
sample queries.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/757701">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>Java concurrency</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Ziyi Lin</a>;
                </span>
                
              </p>
              <p>
                Data /(External/): 
"http:////stap.sjtu.edu.cn//index.php?title=JaConTeBe#Searched/_Bugs"ReferenceStudies
 who have been using the data /(in any form/) are required to include 
the following reference:@INPROCEEDINGS{7372007, author={Lin, Ziyi and 
Marinov, Darko and Zhong, Hao and Chen, Yuting and Zhao, Jianjun}, 
booktitle={Automated Software Engineering /(ASE/), 2015 30th IEEE//ACM 
International Conference on}, title={JaConTeBe: A Benchmark Suite of 
Real-World Java Concurrency Bugs /(T/)}, year={2015}, pages={178-189}, 
keywords={Benchmark testing;Computer bugs;Concurrent computing;Java;Open
 source software;System recovery;JaConTeBe;Java concurrency 
bugs;SIR;benchmark suite;evaluations}, doi={10.1109//ASE.2015.87}, 
month={Nov},}About the DataOverview of DataJaConTeBe is a benchmark 
suite for Java concurrency bugs. The initial version of JaConTeBe 
contains 47 confirmed, real-world concurrency bugs from 8. open-source 
projects. JaConTeBe is an ongoing work and the authors plan to collect 
more bugs in the future.Paper AbstractResearchers have proposed various 
approaches to detect concurrency bugs and improve multi-threaded 
programs, but performing evaluations of the effectiveness of these 
approaches still remains a substantial challenge. We survey the existing
 evaluations and find out that they often use code or bugs not 
representative of real world. To improve representativeness, we have 
prepared JaConTeBe, a benchmark suite of 47 confirmed concurrency bugs 
from 8 popular open-source projects, supplemented with test cases for 
reproducing buggy behaviors. Running three approaches on JaConTeBe shows
 that our benchmark suite confirms some limitations of the three 
approaches. We submitted JaConTeBe to the SIR repository /(a 
software-artifact repository for rigorous controlled experiments/), and 
it was included as a part of SIR.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/2094121">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>Companion page of "Querying Source Code with Natural Language" (Automated Software Engineering 2011)</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Monperrus, Martin</a>;
                </span>
                
              </p>
              <p>
                This is the companion page of the paper /"Querying 
Source Code with Natural Language/".@inproceedings{Kimmig2011, title = 
{Querying Source Code with Natural Language}, author = {Kimmig, Markus 
and Monperrus, Martin and Mezini, Mira}, url = 
{https:////hal.inria.fr//hal-00640496//file//paper.pdf}, booktitle = 
{{26th IEEE//ACM International Conference On Automated Software 
Engineering}}, pages = {376-379}, year = {2011}, doi = 
{10.1109//ASE.2011.6100076},}AbstractOne common task of developing or 
maintaining software is searching the source code for information like 
specific method calls or write accesses to certain fields. This kind of 
information is required to correctly implement new features and to solve
 bugs. For this, development environments offer search tool that are 
based on many radio-buttons and check-boxes. This paper presents an 
approach for querying source code with natural language. It enables the 
developer to execute a huge range of precise searches while being as 
easy and intuitive to use as writing natural language. The evaluation 
shows that the prototype implementation, integrated with the Eclipse 
development environment for Java, supports a wide range of queries and 
is able to correctly understand most real developer queries.Replication 
dataPrototype: training-data.txt: Training data /(annotated queries/) 
mapping-data.txt: Mapping data /(stemmed form -/&gt; JDT Search 
parameter/) unit-tests.csv: Unit test queries Experiment: 
experiment-instructions.pdf: Instructions experiment-tasks.pdf: Task 
Description experiment-project.zip: Input Eclipse Project /(Space 
invader/) experiment-logs.zip: Logged input queries
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/758256">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>Aspect J</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>An Ngoc Lam</a>;
                </span>
                
              </p>
              <p>
                ReferenceStudies who have been using the data /(in any 
form/) are required to include the following 
reference:@INPROCEEDINGS{7372035, author={Lam, An Ngoc and Nguyen, Anh 
Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.}, booktitle={Automated 
Software Engineering /(ASE/), 2015 30th IEEE//ACM International 
Conference on}, title={Combining Deep Learning with Information 
Retrieval to Localize Buggy Files for Bug Reports /(N/)}, year={2015}, 
pages={476-481}, keywords={Bridges;Computer bugs;Feature 
extraction;History;Information retrieval;Metadata;Software;Bug 
Localization;Bug Reports;Deep Learning;Deep Neural Network;Information 
Retrieval}, doi={10.1109//ASE.2015.73}, month={Nov},}About the DataThis 
dataset is one of the Datasets donated by An Ngoc Lam.Overview of 
DataThe data is present in 2 files:“AspectJ.xlsx” : A spreadsheet with 
the bug-ids, commits, its summary, files etc.“AspectJ.xml” : An xml file
 with more detailed information than the above spreadsheet/(detailed 
files changed/).Attribute InformationThe spreadsheet contains a table 
with the “bug/_id”, “summary”, “description”, “time/_reported”, “commit 
associated”, “status of commit” and “files committed”.The xml contains 
the above information and additionally the lines associated with the 
commit.Paper AbstractBug localization refers to the automated process of
 locating the potential buggy files for a given bug report. To help 
developers focus their attention to those files is crucial. Several 
existing automated approaches for bug localization from a bug report 
face a key challenge, called lexical mismatch, in which the terms used 
in bug reports to describe a bug are different from the terms and code 
tokens used in source files. This paper presents a novel approach that 
uses deep neural network /(DNN/) in combination with rVSM, an 
information retrieval /(IR/) technique. rVSM collects the feature on the
 textual similarity between bug reports and source files. DNN is used to
 learn to relate the terms in bug reports to potentially different code 
tokens and terms in source files and documentation if they appear 
frequently enough in the pairs of reports and buggy files. Our empirical
 evaluation on real-world projects shows that DNN and IR complement well
 to each other to achieve higher bug localization accuracy than 
individual models. Importantly, our new model, HyLoc, with a combination
 of the features built from DNN, rVSM, and project’s bug-fixing history,
 achieves higher accuracy than the state-of-the-art IR and machine 
learning techniques. In half of the cases, it is correct with just a 
single suggested file. Two out of three cases, a correct buggy file is 
in the list of three suggested files.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/1040056">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>Data set for the paper Predicting Relevance of Change Recommendations</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Rolfsnes, Thomas</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Moonen, Leon</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Binkley, David</a>;
                </span>
                
              </p>
              <p>
                Data set for the paper Predicting Relevance of Change 
Recommendations by Thomas Rolfsnes, Leon Moonen, and 
David/&nbsp;Binkley, In International Conference on Automated Software 
Engineering /(ASE/),/&nbsp;pp. 694/–705. 2017, IEEE.Please cite this 
work by referring to the corresponding conference publication /(a 
preprint is included in this package/).Abstract: Software change 
recommendation seeks to suggest artifacts /(e.g., files or methods/) 
that are related to changes made by a developer, and thus identifies 
possible omissions or next steps. While one obvious challenge for 
recommender systems is to produce accurate recommendations, a 
complimentary challenge is to rank recommendations based on their 
relevance. In this paper, we address this challenge for recommendation 
systems that are based on evolutionary coupling. Such systems use 
targeted association-rule mining to identify relevant patterns in a 
software system/'s change history. Traditionally, this process involves 
ranking artifacts using interestingness measures such as confidence and 
support. However, these measures often fall short when used to assess 
recommendation relevance. We propose the use of random forest 
classification models to assess recommendation relevance. This approach 
improves on past use of various interestingness measures by learning 
from previous change recommendations. We empirically evaluate our 
approach on fourteen open source systems and two systems from our 
industry partners. Furthermore, we consider complimenting two mining 
algorithms: CO-CHANGE and TARMAQ. The results find that random forest 
classification significantly outperforms previous approaches, receives 
lower Brier scores, and has superior trade-off between precision and 
recall. The results are consistent across software system and mining 
algorithm.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/1341014">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>Continuous Code Quality: Are We (Really) Doing That? Online Appendix</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Vassallo, Carmine</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Palomba, Fabio</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Bacchelli, Alberto</a>;
                </span>
                
                <span class="dataset-author">
                  <a>Gall, Harald C.</a>;
                </span>
                
              </p>
              <p>
                Online appendix for paper /"Continuous Code Quality: Are
 We /(Really/) Doing That?/" by/&nbsp;Carmine Vassallo, Fabio Palomba, 
Alberto Bacchelli, Harald C. Gall. The paper will appear in the 
proceedings of the 33rd IEEE//ACM International Conference on Automated 
Software Engineering /(ASE/),/&nbsp;Montpellier, France, 
2018.Abstract./&nbsp;Continuous Integration /(CI/) is a software 
engineering practice where developers constantly integrate their changes
 to a project through an automated build process. The goal of CI is to 
provide developers with prompt feedback on several quality dimensions 
after each change. Indeed, previous studies provided empirical evidence 
on a/&nbsp;positive association between properly following CI principles
 and source code quality. A core principle behind CI is Continuous Code 
Quality /(also known as CCQ, which includes automated testing and 
automated code inspection/) may appear simple and effective, yet we know
 little about its practical adoption. In this paper, we propose a 
preliminary empirical investigation aimed at understanding 
how/&nbsp;rigorously/&nbsp;practitioners follow CCQ. Our study reveals 
a/&nbsp;strong/&nbsp;dichotomy between theory and practice: developers 
do not perform continuous inspection but rather control for quality only
 at the end of a sprint and most of the times only on the release 
branch.
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/2527909">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>ncrncornell/ced2ar-synlbd-codebook: DDI Codebook for the Synthetic LBD</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Lars Vilhuber</a>;
                </span>
                
              </p>
              <p>
                Codebook for the Synthetic LBD, a Census Bureau data 
product, see https:////www.census.gov//ces//dataproducts//synlbd//.The 
SynLBD usage model relies on a Synthetic Data Server, maintained /(as of
 2018/) by Cornell University, see 
https:////www2.vrdc.cornell.edu//news//synthetic-data-server//.Live 
version of the DDI codebook at 
https:////www2.ncrn.cornell.edu//ced2ar-web//codebooks//synlbd//
              </p>
            </div>
           </div>          
          
          <hr class="no-margin-top">
          <div class="row">     
            <div class="col-md-12">
              <div class="float-right">
                 <a class="btn btn-outline-secondary btn-sm" href="https://zenodo.org/record/758122">View</a>
               </div>
            </div>
            <div class="w"></div>
            <div class="col-md-12">
              <h4>
                <a>usecasedocs</a>
              </h4>
              <p>
                
                <span class="dataset-author">
                  <a>Shuang Liu</a>;
                </span>
                
              </p>
              <p>
                ReferenceStudies who have been using the data /(in any 
form/) are required to include the following 
reference:@inproceedings{Liu:2014:AED:2642937.2642969, author = {Liu, 
Shuang and Sun, Jun and Liu, Yang and Zhang, Yue and Wadhwa, Bimlesh and
 Dong, Jin Song and Wang, Xinyu}, title = {Automatic Early Defects 
Detection in Use Case Documents}, booktitle = {Proceedings of the 29th 
ACM//IEEE International Conference on Automated Software Engineering}, 
series = {ASE '14}, year = {2014}, isbn = {978-1-4503-3013-8}, location =
 {Vasteras, Sweden}, pages = {785--790}, numpages = {6}, url = 
{http:////doi.acm.org//10.1145//2642937.2642969}, doi = 
{10.1145//2642937.2642969}, acmid = {2642969}, publisher = {ACM}, 
address = {New York, NY, USA}, keywords = {natural language processing, 
use cases}, }About the DataOverview of DataIt has been reported that 
“More than 60/% of the errors in a software product are committed during
 the design and less than 40/% during coding.”/[1/] and “Finding and 
fixing a software problem after delivery is often 100 times more 
expensive than finding and fixing it during the requirements and design 
phase” /[2/]. So finding defects in an early stage of software 
development is of great importance.Use cases are widely used in 
Model-Driven Development to capture user requirements. Since the 
majority part of a use case document is written in natural language, it 
is thus highly desirable to rely on advanced natural language processing
 techniques to automatic the procedure of defects detection in use case 
documents.Natural Language ParserZpar is a statistical muti-language 
parser. It has the state-of-the-art speed and accuracy for both Chinese 
and English on standard Penn Treebank data. Zpar provides word 
segmentation, part-of-speech tagging, dependency parsing and phrase 
structure parsing functionalities.Use Case Defect Finder /(UCDF/)We 
developed a tool /(UCDF/) to automatically analysis use case documents 
and find defects. The source code is available here.Input Use Case 
DocumentsWe tested UCDF on two use case documents /(for real systems/). 
One is a stock trading system and the other is a personalized health 
informatics system for a reference implementation for 
IEEEP2407-compliant system. The stock trading system is in real use, 
thus the specifications of the system are confidential. We release the 
use case document for the personalized health informatics system, the 
automated guided vehicle system, the emergency monitoring system and the
 online shopping system.Paper AbstractUse cases, as the primary 
techniques in the user requirement analysis, have been widely adopted in
 the requirement engineering practice. As developed early, use cases 
also serve as the basis for function requirement development, system 
design and testing. Errors in the use cases could potentially lead to 
problems in the system design or implementation. It is thus highly 
desirable to detect errors in use cases. Automatically analyzing use 
case documents is challenging primarily because they are written in 
natural languages. In this work, we aim to achieve automatic defect 
detection in use case documents by leveraging on advanced parsing 
techniques. In our approach, we first parse the use case document using 
dependency parsing techniques. The parsing results of each use case are 
further processed to form an activity diagram. Lastly, we perform defect
 detection on the activity diagrams. To evaluate our approach, we have 
conducted experiments on 200+ real-world as well as academic use cases. 
The results show the effectiveness of our method.
              </p>
            </div>
           </div>          
          
          
          <ul class="pagination justify-content-center"> 
          
          
          
          <li class="page-item">
          
          <a class="page-link" href="hdataset_other_p3.html">
              Previous
          </a>
          </li>
          
          
          
          
          <li class="page-item">
          
          <a class="page-link" href="dataset_other_p1.html">
              1
          </a>
          </li>
          
          
          
          
          <li class="page-item">
          
          <a class="page-link" href="dataset_other_p2.html">
              2
          </a>
          </li>
          
          
          
          
          <li class="page-item">
          
          <a class="page-link" href="dataset_other_p3.html">
              3
          </a>
          </li>
          
          
          
          
          <li class="page-item active">
          
          <a class="page-link" href="dataset_other_p4.html">
              4
          </a>
          </li>
          
          
          
          <li class="page-item disabled"> <a class="page-link" href="#" tabindex="-1"> Next </a> </li>          
          
          
        <ul>
        </ul></ul></div>
      </div>
    </div>
  </section>
</main>

<script src='js/footer.js'></script>